<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>statistics | gbertagnolli</title>
    <link>/tag/statistics/</link>
      <atom:link href="/tag/statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>statistics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>© 2023 Giulia  Bertagnolli</copyright><lastBuildDate>Wed, 08 Mar 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/sharing.png</url>
      <title>statistics</title>
      <link>/tag/statistics/</link>
    </image>
    
    <item>
      <title>Short course: The geometries of statistical models</title>
      <link>/post/2023-03-08-ig/</link>
      <pubDate>Wed, 08 Mar 2023 00:00:00 +0000</pubDate>
      <guid>/post/2023-03-08-ig/</guid>
      <description>&lt;h2 id=&#34;course-description&#34;&gt;Course description&lt;/h2&gt;
&lt;p&gt;Interest in the geometrical approach to statistical inference has been growing in the last decades. Starting from the seminal works of Rao (1945) and then Amari and Chentsov in the 80s, the inter-disciplinary field of Information Geometry (IG) is now well-established with few dedicated conferences and a journal &lt;a href=&#34;https://www.springer.com/journal/41884/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Information Geometry&lt;/a&gt;.
The main objects under study in IG are (i) the manifold of probability distributions and statistical models therein, for instance, a curve on this manifold is a one-dimensional model, and (ii) their invariant and (Riemannian) metric structure. We will see that regular statistical models are Riemannian manifolds, with the Fisher information matrix playing the role of the metric tensor. The Fisher metric, together with a particular pair of dually coupled affine connections, gives the statistical manifold its characteristic dually flat structure, which is at the heart of IG.
Using these classical results in IG we can finally understand, e.g., why the Kullback-Leibler divergence is so useful. Applications of IG are many and diverse, see for instance the &lt;a href=&#34;https://www.dsf.tuhh.de/index.php/ig4ds/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;conference program (and freely available videos of the talks) of IG4DS (2022)&lt;/a&gt;, and we will try to sketch some of them.&lt;/p&gt;
&lt;h2 id=&#34;list-of-topics&#34;&gt;List of topics&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Preliminaries (some concepts and results from differential geometry and geometric analysis)&lt;/li&gt;
&lt;li&gt;The dually flat structure of exponential families&lt;/li&gt;
&lt;li&gt;Non-parametric information geometry&lt;/li&gt;
&lt;li&gt;Applications&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;course-material&#34;&gt;Course material&lt;/h2&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-download  pr-1 fa-fw&#34;&gt;&lt;/i&gt; You can download here the &lt;a href=&#34;/media/An-Introduction-to-Information-Geometry.pdf&#34; target=&#34;_blank&#34;&gt;lecture notes (in progress)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;schedule&#34;&gt;Schedule&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wednesday 8 March 2023, 08:30-10:30 room 7 @ Povo 0&lt;/li&gt;
&lt;li&gt;Wednesday 15 March 2023, 08:30-10:30 room 7 @ Povo 0&lt;/li&gt;
&lt;li&gt;Wednesday 22 March 2023, 08:30-10:30 room 7 @ Povo 0&lt;/li&gt;
&lt;li&gt;Wednesday 29 March 2023, 08:30-10:30 room 7 @ Povo 0&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Amari, S. I., &amp;amp; Nagaoka, H. (2000). Methods of information geometry (Vol. 191). American Mathematical Society.&lt;/li&gt;
&lt;li&gt;Amari, S. I. (1997). Information geometry. Contemporary Mathematics, 203, 81-96.&lt;/li&gt;
&lt;li&gt;Ay, N., Jost, J., Vân Lê, H., &amp;amp; Schwachhöfer, L. (2017). Information geometry (Vol. 64). Cham: Springer.&lt;/li&gt;
&lt;li&gt;Pistone, G. (2019). Information geometry of the probability simplex: A short course. arXiv preprint arXiv:1911.01876.&lt;/li&gt;
&lt;li&gt;Pistone, G. (2013). Nonparametric information geometry. In Geometric Science of Information: First International Conference, GSI 2013, Paris, France, August 28-30, 2013. Proceedings (pp. 5-36). Springer Berlin Heidelberg.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;General references on differential geometry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do Carmo, M. P., &amp;amp; Flaherty Francis, J. (1992). Riemannian geometry (Vol. 6). Boston: Birkhäuser.&lt;/li&gt;
&lt;li&gt;Lang, S. (2012). Fundamentals of differential geometry (Vol. 191). Springer Science &amp;amp; Business Media.&lt;/li&gt;
&lt;li&gt;Petersen, P. (2006). Riemannian geometry (Vol. 171). New York: Springer.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MSc: Complex Networks and Statistical Data Depths</title>
      <link>/post/2018-10-24-msc/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-10-24-msc/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A statistical data depth $d(x, \mathbb{P})$ is a measure of &lt;em&gt;depth&lt;/em&gt; or &lt;em&gt;outlyingness&lt;/em&gt; of a sample $x \in \mathbb{R}^p$ with respect to its underlying distribution $\mathbb{P}$ and it provides a centre-outward ordering of sample points.
It can be used to estimate, non-parametrically, location, scale, skewness an other data characteristics.
The deepest point can be seen as multivariate &lt;a href=&#34;https://www.jstor.org/stable/2674037&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zuo, 2000a&lt;/a&gt; or functional&lt;a href=&#34;https://projecteuclid.org/euclid.ss/1455115914&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nieto-reyes, 2016&lt;/a&gt; extension of the median; the trimmed regions of percentiles.
Therefore depths are extremely important, since they enable us to generalise statistical tools like e.g. boxplots, DD-plots, test of hypothesis to more complex data &lt;a href=&#34;https://projecteuclid.org/euclid.aos/1018031260&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Liu, 1999&lt;/a&gt;.
Local versions of depth are also studied &lt;a href=&#34;https://doi.org/10.1016/j.jspi.2010.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Agostinelli, 2011&lt;/a&gt;.
Defining a depth for networks would mean the possibility to extend to graphs and complex networks these grounded descriptive and inference methods.&lt;/p&gt;
&lt;p&gt;The problem with networks is that they do not come with a space, so before applying them the projected Tukey depth (PTD), a newly defined variation on the well-known halfspace Tukey depth, they need to be embedded in space.
This can be achieved through a classical multidimensional scaling of a distance matrix.
Two options are here available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D_{sp}$, shortest path between pairs of nodes, or&lt;/li&gt;
&lt;li&gt;$D_t$ diffusion distance &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevLett.118.168301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;De Domenico PhysRevLett.118.168301, 2017&lt;/a&gt;, a metric defined as the probability of two random walkers starting from vertices $u, v$ respectively to meet somewhere in the network within the diffusion time $t$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The free parameters are $p$, the dimension of the embedding space and $t$, diffusion time in case diffusion maps embedding.
If $p$ is very small most of the information contained in the data is lost and this results in a random centrality of nodes; whereas if $p$ is too large compared to the number $N$ of nodes the sample becomes excessively diluted in $\mathbb{R}^p$, so there is not enough information for significant inference and all vertices appear to have the same depth, with some negligible fluctuations.
In between we observe a range of values for $p$ for which nodes depth ranking remains stable.
This, together with evaluation of the statistical content of the reduced sample enables us to select a proper embedding dimension.
As shown in &lt;a href=&#34;https://doi.org/10.1016/j.acha.2006.04.006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coifman, 2006&lt;/a&gt;, $t$ plays the role of scale parameter since computing $D_t(u, v)$ involves summing over all paths between $u$ and $v$ of length at most $t$. This was further shown in &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevLett.118.168301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;De Domenico, PhysRevLett.118.168301, 2017&lt;/a&gt; by direct application on complex networks: small values uncover the micro-scale structures and increasing $t$ uncovers the macro-scale.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
