<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>studies | gbertagnolli</title>
    <link>/category/studies/</link>
      <atom:link href="/category/studies/index.xml" rel="self" type="application/rss+xml" />
    <description>studies</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-gb</language><copyright>Â© 2023 Giulia  Bertagnolli</copyright><lastBuildDate>Wed, 24 Oct 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/sharing.png</url>
      <title>studies</title>
      <link>/category/studies/</link>
    </image>
    
    <item>
      <title>MSc: Complex Networks and Statistical Data Depths</title>
      <link>/post/2018-10-24-msc/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      <guid>/post/2018-10-24-msc/</guid>
      <description>&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A statistical data depth $d(x, \mathbb{P})$ is a measure of &lt;em&gt;depth&lt;/em&gt; or &lt;em&gt;outlyingness&lt;/em&gt; of a sample $x \in \mathbb{R}^p$ with respect to its underlying distribution $\mathbb{P}$ and it provides a centre-outward ordering of sample points.
It can be used to estimate, non-parametrically, location, scale, skewness an other data characteristics.
The deepest point can be seen as multivariate &lt;a href=&#34;https://www.jstor.org/stable/2674037&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zuo, 2000a&lt;/a&gt; or functional&lt;a href=&#34;https://projecteuclid.org/euclid.ss/1455115914&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nieto-reyes, 2016&lt;/a&gt; extension of the median; the trimmed regions of percentiles.
Therefore depths are extremely important, since they enable us to generalise statistical tools like e.g. boxplots, DD-plots, test of hypothesis to more complex data &lt;a href=&#34;https://projecteuclid.org/euclid.aos/1018031260&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Liu, 1999&lt;/a&gt;.
Local versions of depth are also studied &lt;a href=&#34;https://doi.org/10.1016/j.jspi.2010.08.001&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Agostinelli, 2011&lt;/a&gt;.
Defining a depth for networks would mean the possibility to extend to graphs and complex networks these grounded descriptive and inference methods.&lt;/p&gt;
&lt;p&gt;The problem with networks is that they do not come with a space, so before applying them the projected Tukey depth (PTD), a newly defined variation on the well-known halfspace Tukey depth, they need to be embedded in space.
This can be achieved through a classical multidimensional scaling of a distance matrix.
Two options are here available:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$D_{sp}$, shortest path between pairs of nodes, or&lt;/li&gt;
&lt;li&gt;$D_t$ diffusion distance &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevLett.118.168301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;De Domenico PhysRevLett.118.168301, 2017&lt;/a&gt;, a metric defined as the probability of two random walkers starting from vertices $u, v$ respectively to meet somewhere in the network within the diffusion time $t$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The free parameters are $p$, the dimension of the embedding space and $t$, diffusion time in case diffusion maps embedding.
If $p$ is very small most of the information contained in the data is lost and this results in a random centrality of nodes; whereas if $p$ is too large compared to the number $N$ of nodes the sample becomes excessively diluted in $\mathbb{R}^p$, so there is not enough information for significant inference and all vertices appear to have the same depth, with some negligible fluctuations.
In between we observe a range of values for $p$ for which nodes depth ranking remains stable.
This, together with evaluation of the statistical content of the reduced sample enables us to select a proper embedding dimension.
As shown in &lt;a href=&#34;https://doi.org/10.1016/j.acha.2006.04.006&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Coifman, 2006&lt;/a&gt;, $t$ plays the role of scale parameter since computing $D_t(u, v)$ involves summing over all paths between $u$ and $v$ of length at most $t$. This was further shown in &lt;a href=&#34;https://link.aps.org/doi/10.1103/PhysRevLett.118.168301&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;De Domenico, PhysRevLett.118.168301, 2017&lt;/a&gt; by direct application on complex networks: small values uncover the micro-scale structures and increasing $t$ uncovers the macro-scale.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
